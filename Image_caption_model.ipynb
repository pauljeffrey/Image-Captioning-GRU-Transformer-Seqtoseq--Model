{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Image_caption_model.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1Gxy8cnYhHVt2D9xpxqNXen_igemkWhTo","authorship_tag":"ABX9TyNRvRJPxd1I4cXyKZanJFk8"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"PjxOVWazEvTW"},"source":["import tensorflow as tf\n","\n","\n","import matplotlib.pyplot as plt\n","\n","from sklearn.model_selection import train_test_split\n","from sklearn.utils import shuffle\n","\n","import collections\n","import random\n","import re \n","import numpy as np\n","import os\n","import time\n","import json\n","from glob import glob\n","from PIL import Image\n","import pickle"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FEOgaOCchaii"},"source":["embedding_dim = 256\n","BUFFER_SIZE = 1000\n","units = 512\n","features_shape  = 2048\n","attention_features_shape = 64"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yjX-WQB9nh3V"},"source":["# This model is an image caption model using GRU and attention to generates caption.\n","# This file contains the implementation of the model components only.\n","# It uses the encoder-decoder architecture to generate captions.\n","# The encoder uses a pretrained CNN model to extract features from image before feeding it to a fully connected layer that reduces its dimensions\n","# The encoder output is processed by the decoder using an attention mechanism that weights the features before it is finally processed by a GRU\n","# and two fully connected layers to generate captions.\n","\n","\n","class Attention(tf.keras.Model):\n","  # Attention mechanism implementation\n","  def __init__(self, units):\n","    super(Attention, self).__init__()\n","    self.W1 = tf.keras.layers.Dense(units)\n","    self.W2 = tf.keras.layers.Dense(units)\n","    self.V = tf.keras.layers.Dense(1)\n","\n","  def call(self, features, hidden):\n","    # features(CNN_encoder output) shape == (batch_size, 64, embedding_dim)\n","\n","    # hidden shape = (batch_size  , hidden_size)\n","    # hidden_with_time_axis_shape ==  (batch_size, 1, hidden_size)\n","    hidden_with_time_axis = tf.expand_dims(hidden,1)\n","\n","    #attention_hidden_layer shape == (batch_size, 64, units)\n","    attention_hidden_layer  = (tf.nn.tanh(self.W1(features) + self.W2(hidden_with_time_axis)))\n","\n","    # score shape == (batch_size, 64, 1)\n","    # This gives you an unnormalized score for each image feature.\n","    score = self.V(attention_hidden_layer)\n","\n","    # attention_weights_shape == (batch_size, 64, 1)\n","    attention_weights = tf.nn.softmax(score, axis=1)\n","\n","    # context_vector shape after sum == (batch_size, hidden_size ) / ( batch_size , 64 ,1)\n","    context_vector = attention_weights * features\n","    context_vector = tf.reduce_sum(context_vector, axis=1)\n","\n","    return context_vector, attention_weights"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oaa0AV8Vqmf5"},"source":["class CNN_Encoder(tf.keras.Model):\n","  # Since features have already been extracted and dumped it using pickle\n","  # This encoder passes those features through a fully connected layer\n","  def __init__(self, embedding_dim):\n","    super(CNN_Encoder, self).__init__()\n","    # shape after fc == (batch_size, 64, embedding_dim)\n","    self.fc = tf.keras.layers.Dense(embedding_dim)\n","\n","  def call(self,x):\n","    x = self.fc(x)\n","    x = tf.nn.relu(x)\n","    return x"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sVI2jQ9ZrTXc"},"source":["class RNN_Decoder(tf.keras.Model):\n","  def __init__(self, embedding_dim, units, vocab_size):\n","    super(RNN_Decoder, self).__init__()\n","    self.units = units\n","\n","    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n","    self.GRU = tf.keras.layers.GRU(self.units,\n","                                   return_sequences = True,\n","                                   return_state = True,\n","                                   recurrent_initializer = 'glorot_uniform')\n","    \n","    self.fc1 = tf.keras.layers.Dense(self.units)\n","    self.fc2 = tf.keras.layers.Dense(vocab_size)\n","\n","    self.attention  = Attention(self.units)\n","\n","  def call(self,x, features ,hidden):\n","    # defining attention as a separate model\n","    context_vector, attention_weights = self.attention(features, hidden)\n","\n","    # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n","    x = self.embedding(x)\n","\n","    # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n","    x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n","\n","    # passing the concatenated vector to the GRU\n","    output, state = self.GRU(x)\n","\n","    # x shape == (batch_size , max_length, hidden_size) / (batch_size,1,64)\n","    x = self.fc1(output)\n","\n","    # x shape == (batch_size * max_length, hidden_size)\n","    x = tf.reshape(x, (-1, x.shape[2]))\n","\n","    # output shape == (batch_size * max_length, vocab)\n","    x = self.fc2(x)\n","\n","    return x, state ,attention_weights\n","\n","  def reset_state(self, batch_size):\n","    return tf.zeros((batch_size, self.units))\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hXmQ4txbv8Cn"},"source":["optimizer = tf.keras.optimizers.Adam()\n","loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits =True, reduction='none')\n","\n","def loss_function(real, pred):\n","  mask = tf.math.logical_not(tf.math.equal(real, 0))\n","  loss = loss_object(real,pred)\n","\n","  mask  = tf.cast(mask, dtype=loss_.dtype)\n","  loss_ *= mask\n","\n","  return tf.reduce_mean(loss_)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XA-_ka7GwqMf"},"source":["def checkpoint_manager(encoder, decoder, optimizer,path = './checkpoints/train'):\n","  ckpt = tf.train.Checkpoint(encoder = encoder,\n","                             decoder = decoder,\n","                             optimizer = optimizer)\n","  ckpt_manager = tf.train.CheckpointManager(ckpt, path, max_to_keep=5)\n","\n","  return ckpt , ckpt_manager"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qoNP24aDx-VR"},"source":["def restore_checkpoint(ckpt, ckpt_manager):\n","  start_epoch = 0\n","  if ckpt_manager.latest_checkpoint:\n","    start_epoch = int(ckpt_manager.latest_checkpoint.split('-')[-1])\n","    # restoring the latest checkpoint in checkpoint_path\n","    ckpt.restore(ckpt_manager.latest_checkpoint)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bxa7ro1Kywwh"},"source":["@tf.function\n","def train_step(img_tensor, target):\n","  loss = 0\n","\n","  # initializing the hidden state for each batch\n","  # because the captions are not related from image to image\n","  hidden = decoder.reset_state(batch_size=target.shape[0])\n","\n","  dec_input = tf.expand_dims([tokenizer.word_index['<start>']] * target.shape[0], 1)\n","\n","  with tf.GradientTape() as tape:\n","    features = encoder(img_tensor)\n","\n","    for i in range(1, target.shape[1]):\n","      # passing the features through the decoder\n","      predictions, hidden, _ = decoder(dec_input, features , hidden)\n","\n","      loss += loss_function(target[:, i], predictions)\n","\n","      #using teacher forcing\n","      dec_input = tf.expand_dims(target[:, i], 1)\n","\n","  total_loss = (loss / int(target.shape[1]))\n","\n","  trainable_variables = encoder.trainable_variables + decoder.trainable_variables\n","\n","  gradients = tape.gradient(loss, trainable_variables)\n","\n","  optimizer.apply_gradients(zip(gradients, trainable_variables))\n","\n","  return loss, total_loss\n","\n","\n","def train(dataset, num_steps, ckpt_manager, start_epoch= 0, epochs=25):\n","  # Customized to train from latest checkpoint\n","\n","  loss_plot = []\n","  for epoch in range(start_epoch, epochs):\n","    start = time.time()\n","    total_loss = 0\n","\n","    for (batch, (img_tensor, target)) in enumerate(dataset):\n","      batch_loss , t_loss = train_step(img_tensor, target)\n","      total_loss += t_loss\n","\n","      if batch % 100 == 0:\n","        print('Epoch {} Batch {} Loss {:.4f}'.format(epoch +1, batch, batch_loss.numpy() / int(target.shape[1])))\n","\n","    # storing the epoch end loss value to plot later \n","    loss_plot.append(total_loss / num_steps)\n","\n","    if epoch % 5 == 0:\n","      ckpt_manager.save()\n","    \n","    print( 'Epoch {} Loss {:.6f}'.format(epoch + 1, total_loss/ num_steps))\n","\n","    print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))\n","    # Do num_stepps = train_size/ batch _size"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wi72QuNEk1L4"},"source":["def plot_loss(loss_plot):\n","  plt.plot(loss_plot)\n","  plt.xlabel('Epochs')\n","  plt.ylabel('Loss')\n","  plt.title('Loss Plot')\n","  plt.show()\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_Qt9K6_jvW7i"},"source":["def load_model():\n","  # fuction loads the pretrained inception model\n","  image_model = tf.keras.applications.InceptionV3(include_top=False, weights='imagenet')\n","\n","  new_input = image_model.input\n","  hidden_layer = image_model.layers[-1].output\n","\n","  image_features_extract_model = tf.keras.Model(new_input, hidden_layer)\n","\n","  return image_features_extract_model\n","\n","\n","def load_image(image_path, res_w, res_h, channels=3):\n","  # function loads image , resizes it to the input dimension shape of the pretrained inception model and then processes the image using the inception model.\n","  img = tf.io.read_file(image_path)\n","  img = tf.image.decode_jpeg(img, channels= channels)\n","  img = tf.image.resize(img, (res_w, res_h))\n","  img = tf.keras.applications.inception_v3.preprocess_input(img)\n","  return img, image_path"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-EWR0_6JlmO0"},"source":["\n","def generate_caption(image,image_features_extract_model, encoder,decoder, max_length):\n","  # This function is customized to generate captions using the 'teacher forcing' mechanism.\n","  attention_plot = np.zeros((max_length, attention_features_shape))\n","\n","  hidden = decoder.reset_state(batch_size=1)\n","\n","  temp_input = tf.expand_dims(load_image(image)[0], 0) # Define load_image\n","  img_tensor_val = image_features_extract_model(temp_input)\n","  img_tensor_val = tf.reshape(img_tensor_val, (img_tensor_val.shape[0], -1, img_tensor_val.shape[3]))\n","\n","  features = encoder(img_tensor_val) # encoder output\n","\n","  dec_input = tf.expand_dims([tokenizer.word_index['<start>']], 0)\n","  result = []\n","\n","  for i in range(max_length):\n","    predictions , hidden , attention_weights  = decoder(dec_input, features, hidden) # decoder output\n","\n","    attention_plot[i] = tf.reshape(attention_weights, (-1, )).numpy() # save attention weights in attention_plot dict\n","\n","    predicted_id = tf.random.categorical(predictions, 1)[0][0].numpy()# get token\n","\n","    result.append(tokenizer.index_word[predicted_id])\n","\n","    if tokenizer.index_word[predicted_id] == '<end>':\n","      return result, attention_plot\n","\n","    dec_input = tf.expand_dims([predicted_id], 0)\n","\n","  attention_plot = attention_plot[: len(result), :]\n","  return result, attention_plot"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LPK90GK9skSM"},"source":["def plot_attention(image, result, attention_plot):\n","  # Function is used to create attention plots on images during caption generation.\n","  temp_image = np.array(Image.open(image))\n","\n","  fig = plt.figure(figsize=(10,10))\n","\n","  len_result = len(result)\n","\n","  for l in range(len(result)):\n","    temp_att = np.resize(attention_plot[l], (8,8))\n","    ax = fig.add_subplot(len_result//2, len_result//2, l+1)\n","    ax.set_title(result[l])\n","    img = ax.imshow(temp_image)\n","    ax.imshow(temp_att, cmap='gray', alpha=0.6, extent=img.get_extent())\n","\n","  plt.tight_layout()\n","  plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qH6ABV13UYi_"},"source":[""],"execution_count":null,"outputs":[]}]}